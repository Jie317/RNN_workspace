{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this tutorial I’ll explain how to build a simple working \n",
    "Recurrent Neural Network in TensorFlow! \n",
    "\n",
    "We will build a simple Echo-RNN that remembers the input sequence and then echoes it after a few time-steps. This will help us understand how\n",
    "memory works \n",
    "\n",
    "We are mapping two sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# What is an RNN?\n",
    "It is short for “Recurrent Neural Network”, and is basically a neural \n",
    "network that can be used when your data is treated as a sequence, where \n",
    "the particular order of the data-points matter. More importantly, this \n",
    "sequence can be of arbitrary length.\n",
    "\n",
    "The most straight-forward example is perhaps a time-seriedems of numbers, \n",
    "where the task is to predict the next value given previous values. The \n",
    "input to the RNN at every time-step is the current value as well as a \n",
    "state vector which represent what the network has “seen” at time-steps \n",
    "before. This state-vector is the encoded memory of the RNN, initially \n",
    "set to zero.\n",
    "\n",
    "Great paper on this \n",
    "https://arxiv.org/pdf/1506.00019.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# Image(url= \"https://cdn-images-1.medium.com/max/1600/1*UkI9za9zTR-HL8uM15Wmzw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 1, 1, ..., 1, 0, 1],\n",
      "       [1, 0, 1, ..., 1, 0, 1],\n",
      "       [1, 1, 1, ..., 1, 0, 0],\n",
      "       [0, 1, 1, ..., 0, 0, 1],\n",
      "       [1, 1, 1, ..., 0, 1, 0]]), array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [1, 0, 1, ..., 0, 0, 1],\n",
      "       [1, 0, 1, ..., 0, 1, 0],\n",
      "       [1, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 1, 0]]))\n"
     ]
    }
   ],
   "source": [
    "#Step 1 - Collect data\n",
    "#Now generate the training data, \n",
    "#the input is basically a random binary vector. The output will be the \n",
    "#“echo” of the input, shifted echo_step steps to the right.\n",
    "\n",
    "#Notice the reshaping of the data into a matrix with batch_size rows. \n",
    "#Neural networks are trained by approximating the gradient of loss function \n",
    "#with respect to the neuron-weights, by looking at only a small subset of the data, \n",
    "#also known as a mini-batch.The reshaping takes the whole dataset and puts it into \n",
    "#a matrix, that later will be sliced up into these mini-batches.\n",
    "\n",
    "def generateData():\n",
    "    #0,1, 50K samples, 50% chance each chosen\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    #shift 3 steps to the left\n",
    "    y = np.roll(x, echo_step)\n",
    "    #padd beginning 3 values with 0\n",
    "    y[0:echo_step] = 0\n",
    "    #Gives a new shape to an array without changing its data.\n",
    "    #The reshaping takes the whole dataset and puts it into a matrix, \n",
    "    #that later will be sliced up into these mini-batches.\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "data = generateData()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*aFtwuFsboLV8z5PkEzNLXA.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Schematic of the reshaped data-matrix, arrow curves shows adjacent time-steps that ended up on different rows. \n",
    "#Light-gray rectangle represent a “zero” and dark-gray a “one”.\n",
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*aFtwuFsboLV8z5PkEzNLXA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TensorFlow works by first building up a computational graph, that \n",
    "#specifies what operations will be done. The input and output of this graph\n",
    "#is typically multidimensional arrays, also known as tensors. \n",
    "#The graph, or parts of it can then be executed iteratively in a \n",
    "#session, this can either be done on the CPU, GPU or even a resource \n",
    "#on a remote server.\n",
    "\n",
    "#operations and tensors\n",
    "\n",
    "#The two basic TensorFlow data-structures that will be used in this \n",
    "#example are placeholders and variables. On each run the batch data \n",
    "#is fed to the placeholders, which are “starting nodes” of the \n",
    "#computational graph. Also the RNN-state is supplied in a placeholder, \n",
    "#which is saved from the output of the previous run.\n",
    "\n",
    "#Step 2 - Build the Model\n",
    "\n",
    "#datatype, shape (5, 15) 2D array or matrix, batch size shape for later\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "#and one for the RNN state, 5,4 \n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The weights and biases of the network are declared as TensorFlow vari*ables,\n",
    "#which makes them persistent across runs and enables them to be updated\n",
    "#incrementally for each batch.\n",
    "\n",
    "#3 layer recurrent net, one hidden state\n",
    "\n",
    "#randomly initialize weights\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "#anchor, improves convergance, matrix of 0s \n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The figure below shows the input data-matrix, and the current batch batchX_placeholder \n",
    "is in the dashed rectangle. As we will see later, this “batch window” is slided truncated_backprop_length \n",
    "steps to the right at each run, hence the arrow. In our example below batch_size = 3, truncated_backprop_length = 3, \n",
    "and total_series_length = 36. Note that these numbers are just for visualization purposes, the values are different in the code. \n",
    "The series order index is shown as numbers in a few of the data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*n45uYnAfTDrBvG87J-poCA.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*n45uYnAfTDrBvG87J-poCA.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Now it’s time to build the part of the graph that resembles the actual RNN computation, \n",
    "#first we want to split the batch data into adjacent time-steps.\n",
    "\n",
    "# Unpack columns\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#so a bunch of arrays, 1 batch per time step\n",
    "inputs_series = tf.unpack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unpack(batchY_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see in the picture below that is done by unpacking the columns (axis = 1) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. The reason for using the variable names “plural”_”series” is to emphasize that the variable is a list that represent a time-series with multiple entries at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*f2iL4zOkBUBGOpVE7kyajg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*f2iL4zOkBUBGOpVE7kyajg.png\")\n",
    "#Schematic of the current batch split into columns, the order index is shown on each data-point \n",
    "#and arrows show adjacent time-steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The fact that the training is done on three places simultaneously in our time-series, requires us to save three instances of states when propagating forward. That has already been accounted for, as you see that the init_state placeholder has batch_size rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Forward pass\n",
    "#state placeholder\n",
    "current_state = init_state\n",
    "#series of states through time\n",
    "states_series = []\n",
    "\n",
    "\n",
    "#for each set of inputs\n",
    "#forward pass through the network to get new state value\n",
    "#store all states in memory\n",
    "for current_input in inputs_series:\n",
    "    #format input\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    #mix both state and input data \n",
    "    input_and_state_concatenated = tf.concat(1, [current_input, current_state])  # Increasing number of columns\n",
    "    #perform matrix multiplication between weights and input, add bias\n",
    "    #squash with a nonlinearity, for probabiolity value\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    #store the state in memory\n",
    "    states_series.append(next_state)\n",
    "    #set current state to next one\n",
    "    current_state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice the concatenation on line 6, what we actually want to do is calculate the sum of two affine transforms current_input * Wa + current_state * Wb in the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias b is broadcasted on all samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You may wonder the variable name truncated_backprop_length is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#calculate loss\n",
    "#second part of forward pass\n",
    "#logits short for logistic transform\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "#apply softmax nonlinearity for output probability\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "#measure loss, calculate softmax again on logits, then compute cross entropy\n",
    "#measures the difference between two probability distributions\n",
    "#this will return A Tensor of the same shape as labels and of the same type as logits \n",
    "#with the softmax cross entropy loss.\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "#computes average, one value\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "#use adagrad to minimize with .3 learning rate\n",
    "#minimize it with adagrad, not SGD\n",
    "#One downside of SGD is that it is sensitive to\n",
    "#the learning rate hyper-parameter. When the data are sparse and features have\n",
    "#different frequencies, a single learning rate for every weight update can have\n",
    "#exponential regret.\n",
    "#Some features can be extremely useful and informative to an optimization problem but \n",
    "#they may not show up in most of the training instances or data. If, when they do show up, \n",
    "#they are weighted equally in terms of learning rate as a feature that has shown up hundreds \n",
    "#of times we are practically saying that the influence of such features means nothing in the \n",
    "#overall optimization. it's impact per step in the stochastic gradient descent will be so small \n",
    "#that it can practically be discounted). To counter this, AdaGrad makes it such that features \n",
    "#that are more sparse in the data have a higher learning rate which translates into a larger \n",
    "#update for that feature\n",
    "#sparse features can be very useful.\n",
    "#Each feature has a different learning rate which is adaptable. \n",
    "#gives voice to the little guy who matters a lot\n",
    "#weights that receive high gradients will have their effective learning rate reduced, \n",
    "#while weights that receive small or infrequent updates will have their effective learning rate increased. \n",
    "#great paper http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically — the computation graph is executed once for each mini-batch and the network-weights are updated incrementally.\n",
    "\n",
    "Notice the API call to sparse_softmax_cross_entropy_with_logits, it automatically calculates the softmax internally and then computes the cross-entropy. In our example the classes are mutually exclusive (they are either zero or one), which is the reason for using the “Sparse-softmax”, you can read more about it in the API. The usage is to havelogits is of shape [batch_size, num_classes] and labels of shape [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#visualizer\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There is a visualization function so we can se what’s going on in the network as we train. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-ae0feeafb530>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a2485fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.697609\n",
      "Step 100 Loss 0.709574\n",
      "Step 200 Loss 0.694296\n",
      "Step 300 Loss 0.689088\n",
      "Step 400 Loss 0.690956\n",
      "Step 500 Loss 0.689938\n",
      "Step 600 Loss 0.688795\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.692611\n",
      "Step 100 Loss 0.699064\n",
      "Step 200 Loss 0.694441\n",
      "Step 300 Loss 0.696779\n",
      "Step 400 Loss 0.691704\n",
      "Step 500 Loss 0.696406\n",
      "Step 600 Loss 0.686523\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.687716\n",
      "Step 100 Loss 0.689138\n",
      "Step 200 Loss 0.704676\n",
      "Step 300 Loss 0.693083\n",
      "Step 400 Loss 0.696722\n",
      "Step 500 Loss 0.461525\n",
      "Step 600 Loss 0.30241\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.190552\n",
      "Step 100 Loss 0.0422117\n",
      "Step 200 Loss 0.0231688\n",
      "Step 300 Loss 0.0166914\n",
      "Step 400 Loss 0.0130415\n",
      "Step 500 Loss 0.0098387\n",
      "Step 600 Loss 0.00735914\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.168415\n",
      "Step 100 Loss 0.0179416\n",
      "Step 200 Loss 0.00578878\n",
      "Step 300 Loss 0.00848532\n",
      "Step 400 Loss 0.00613118\n",
      "Step 500 Loss 0.00407557\n",
      "Step 600 Loss 0.00442893\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.163746\n",
      "Step 100 Loss 0.00343454\n",
      "Step 200 Loss 0.00524622\n",
      "Step 300 Loss 0.00291018\n",
      "Step 400 Loss 0.00295292\n",
      "Step 500 Loss 0.00316489\n",
      "Step 600 Loss 0.00288882\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.129444\n",
      "Step 100 Loss 0.00255715\n",
      "Step 200 Loss 0.00210952\n",
      "Step 300 Loss 0.00200739\n",
      "Step 400 Loss 0.00281304\n",
      "Step 500 Loss 0.00180272\n",
      "Step 600 Loss 0.00145558\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.200301\n",
      "Step 100 Loss 0.00184514\n",
      "Step 200 Loss 0.00177459\n",
      "Step 300 Loss 0.00162197\n",
      "Step 400 Loss 0.00156359\n",
      "Step 500 Loss 0.00159224\n",
      "Step 600 Loss 0.00153606\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.200091\n",
      "Step 100 Loss 0.00191296\n",
      "Step 200 Loss 0.00152011\n",
      "Step 300 Loss 0.00164422\n",
      "Step 400 Loss 0.00154281\n",
      "Step 500 Loss 0.00126609\n",
      "Step 600 Loss 0.00124126\n",
      "New data, epoch 9\n",
      "Step 0 Loss 0.253373\n",
      "Step 100 Loss 0.00116351\n",
      "Step 200 Loss 0.00108913\n",
      "Step 300 Loss 0.000985533\n",
      "Step 400 Loss 0.000983919\n",
      "Step 500 Loss 0.0014546\n",
      "Step 600 Loss 0.000954873\n",
      "New data, epoch 10\n",
      "Step 0 Loss 0.253324\n",
      "Step 100 Loss 0.00109409\n",
      "Step 200 Loss 0.00124358\n",
      "Step 300 Loss 0.000896131\n",
      "Step 400 Loss 0.000891142\n",
      "Step 500 Loss 0.00110932\n",
      "Step 600 Loss 0.00087129\n",
      "New data, epoch 11\n",
      "Step 0 Loss 0.272643\n",
      "Step 100 Loss 0.00124967\n",
      "Step 200 Loss 0.00115775\n",
      "Step 300 Loss 0.000755694\n",
      "Step 400 Loss 0.000969947\n",
      "Step 500 Loss 0.00104147\n",
      "Step 600 Loss 0.000932666\n",
      "New data, epoch 12\n",
      "Step 0 Loss 0.246009\n",
      "Step 100 Loss 0.00102001\n",
      "Step 200 Loss 0.00112928\n",
      "Step 300 Loss 0.00105177\n",
      "Step 400 Loss 0.000791862\n",
      "Step 500 Loss 0.00073598\n",
      "Step 600 Loss 0.000589278\n",
      "New data, epoch 13\n",
      "Step 0 Loss 0.0998127\n",
      "Step 100 Loss 0.000712649\n",
      "Step 200 Loss 0.00075555\n",
      "Step 300 Loss 0.000577148\n",
      "Step 400 Loss 0.000682808\n",
      "Step 500 Loss 0.00065077\n",
      "Step 600 Loss 0.000517365\n",
      "New data, epoch 14\n",
      "Step 0 Loss 0.277322\n",
      "Step 100 Loss 0.000923688\n",
      "Step 200 Loss 0.000796875\n",
      "Step 300 Loss 0.000569587\n",
      "Step 400 Loss 0.000629627\n",
      "Step 500 Loss 0.000647546\n",
      "Step 600 Loss 0.000688254\n",
      "New data, epoch 15\n",
      "Step 0 Loss 0.241591\n",
      "Step 100 Loss 0.00243958\n",
      "Step 200 Loss 0.00157773\n",
      "Step 300 Loss 0.00109305\n",
      "Step 400 Loss 0.00117791\n",
      "Step 500 Loss 0.00111165\n",
      "Step 600 Loss 0.000907108\n",
      "New data, epoch 16\n",
      "Step 0 Loss 0.150208\n",
      "Step 100 Loss 0.000948114\n",
      "Step 200 Loss 0.000830345\n",
      "Step 300 Loss 0.00067923\n",
      "Step 400 Loss 0.000858164\n",
      "Step 500 Loss 0.000860592\n",
      "Step 600 Loss 0.000731174\n",
      "New data, epoch 17\n",
      "Step 0 Loss 0.164423\n",
      "Step 100 Loss 0.000864484\n",
      "Step 200 Loss 0.000775389\n",
      "Step 300 Loss 0.00090431\n",
      "Step 400 Loss 0.000858771\n",
      "Step 500 Loss 0.000659477\n",
      "Step 600 Loss 0.000683405\n",
      "New data, epoch 18\n",
      "Step 0 Loss 0.169909\n",
      "Step 100 Loss 0.000832698\n",
      "Step 200 Loss 0.000858982\n",
      "Step 300 Loss 0.000769949\n",
      "Step 400 Loss 0.000550734\n",
      "Step 500 Loss 0.000534477\n",
      "Step 600 Loss 0.000642735\n",
      "New data, epoch 19\n",
      "Step 0 Loss 0.301759\n",
      "Step 100 Loss 0.000602911\n",
      "Step 200 Loss 0.000626348\n",
      "Step 300 Loss 0.000522947\n",
      "Step 400 Loss 0.000512124\n",
      "Step 500 Loss 0.000632763\n",
      "Step 600 Loss 0.000631393\n",
      "New data, epoch 20\n",
      "Step 0 Loss 0.290447\n",
      "Step 100 Loss 0.000586624\n",
      "Step 200 Loss 0.00045811\n",
      "Step 300 Loss 0.000632011\n",
      "Step 400 Loss 0.000462427\n",
      "Step 500 Loss 0.000494497\n",
      "Step 600 Loss 0.000467322\n",
      "New data, epoch 21\n",
      "Step 0 Loss 0.271325\n",
      "Step 100 Loss 0.000577323\n",
      "Step 200 Loss 0.000400538\n",
      "Step 300 Loss 0.000400108\n",
      "Step 400 Loss 0.000425203\n",
      "Step 500 Loss 0.000403385\n",
      "Step 600 Loss 0.000453507\n",
      "New data, epoch 22\n",
      "Step 0 Loss 0.211613\n",
      "Step 100 Loss 0.000497565\n",
      "Step 200 Loss 0.000449468\n",
      "Step 300 Loss 0.000421379\n",
      "Step 400 Loss 0.000398657\n",
      "Step 500 Loss 0.000375897\n",
      "Step 600 Loss 0.000477074\n",
      "New data, epoch 23\n",
      "Step 0 Loss 0.168917\n",
      "Step 100 Loss 0.00048328\n",
      "Step 200 Loss 0.000472796\n",
      "Step 300 Loss 0.000351409\n",
      "Step 400 Loss 0.00045008\n",
      "Step 500 Loss 0.000308799\n",
      "Step 600 Loss 0.000493969\n",
      "New data, epoch 24\n",
      "Step 0 Loss 0.134626\n",
      "Step 100 Loss 0.000361858\n",
      "Step 200 Loss 0.000458089\n",
      "Step 300 Loss 0.000341519\n",
      "Step 400 Loss 0.000406416\n",
      "Step 500 Loss 0.000365699\n",
      "Step 600 Loss 0.000394228\n",
      "New data, epoch 25\n",
      "Step 0 Loss 0.240375\n",
      "Step 100 Loss 0.00039608\n",
      "Step 200 Loss 0.000414227\n",
      "Step 300 Loss 0.000401746\n",
      "Step 400 Loss 0.000357213\n",
      "Step 500 Loss 0.000356811\n",
      "Step 600 Loss 0.000466286\n",
      "New data, epoch 26\n",
      "Step 0 Loss 0.222307\n",
      "Step 100 Loss 0.00037013\n",
      "Step 200 Loss 0.000447492\n",
      "Step 300 Loss 0.000388597\n",
      "Step 400 Loss 0.000311036\n",
      "Step 500 Loss 0.000321619\n",
      "Step 600 Loss 0.00033387\n",
      "New data, epoch 27\n",
      "Step 0 Loss 0.231393\n",
      "Step 100 Loss 0.000452739\n",
      "Step 200 Loss 0.000430321\n",
      "Step 300 Loss 0.000317814\n",
      "Step 400 Loss 0.000293224\n",
      "Step 500 Loss 0.000298246\n",
      "Step 600 Loss 0.000356282\n",
      "New data, epoch 28\n",
      "Step 0 Loss 0.15208\n",
      "Step 100 Loss 0.000333724\n",
      "Step 200 Loss 0.000341093\n",
      "Step 300 Loss 0.000250767\n",
      "Step 400 Loss 0.000336866\n",
      "Step 500 Loss 0.000327542\n",
      "Step 600 Loss 0.00035486\n",
      "New data, epoch 29\n",
      "Step 0 Loss 0.253987\n",
      "Step 100 Loss 0.000417125\n",
      "Step 200 Loss 0.000363387\n",
      "Step 300 Loss 0.000393496\n",
      "Step 400 Loss 0.000297087\n",
      "Step 500 Loss 0.000278346\n",
      "Step 600 Loss 0.000307302\n",
      "New data, epoch 30\n",
      "Step 0 Loss 0.120254\n",
      "Step 100 Loss 0.00038644\n",
      "Step 200 Loss 0.000325961\n",
      "Step 300 Loss 0.000380578\n",
      "Step 400 Loss 0.000256684\n",
      "Step 500 Loss 0.000229784\n",
      "Step 600 Loss 0.000236436\n",
      "New data, epoch 31\n",
      "Step 0 Loss 0.102149\n",
      "Step 100 Loss 0.000328013\n",
      "Step 200 Loss 0.000252268\n",
      "Step 300 Loss 0.000256645\n",
      "Step 400 Loss 0.000313823\n",
      "Step 500 Loss 0.000247072\n",
      "Step 600 Loss 0.00023938\n",
      "New data, epoch 32\n",
      "Step 0 Loss 0.17848\n",
      "Step 100 Loss 0.000209651\n",
      "Step 200 Loss 0.000225753\n",
      "Step 300 Loss 0.000257613\n",
      "Step 400 Loss 0.00024713\n",
      "Step 500 Loss 0.000243507\n",
      "Step 600 Loss 0.000242206\n",
      "New data, epoch 33\n",
      "Step 0 Loss 0.145164\n",
      "Step 100 Loss 0.000316365\n",
      "Step 200 Loss 0.000272337\n",
      "Step 300 Loss 0.000300319\n",
      "Step 400 Loss 0.000244847\n",
      "Step 500 Loss 0.000293127\n",
      "Step 600 Loss 0.000296398\n",
      "New data, epoch 34\n",
      "Step 0 Loss 0.146345\n",
      "Step 100 Loss 0.000326442\n",
      "Step 200 Loss 0.000315929\n",
      "Step 300 Loss 0.000529314\n",
      "Step 400 Loss 0.000332705\n",
      "Step 500 Loss 0.000301522\n",
      "Step 600 Loss 0.00022392\n",
      "New data, epoch 35\n",
      "Step 0 Loss 0.115029\n",
      "Step 100 Loss 0.000228061\n",
      "Step 200 Loss 0.000229091\n",
      "Step 300 Loss 0.000201189\n",
      "Step 400 Loss 0.000227923\n",
      "Step 500 Loss 0.000225632\n",
      "Step 600 Loss 0.000297741\n",
      "New data, epoch 36\n",
      "Step 0 Loss 0.137963\n",
      "Step 100 Loss 0.000295084\n",
      "Step 200 Loss 0.000212825\n",
      "Step 300 Loss 0.000214485\n",
      "Step 400 Loss 0.000182764\n",
      "Step 500 Loss 0.000237382\n",
      "Step 600 Loss 0.000190176\n",
      "New data, epoch 37\n",
      "Step 0 Loss 0.218574\n",
      "Step 100 Loss 0.000258578\n",
      "Step 200 Loss 0.000249309\n",
      "Step 300 Loss 0.000263167\n",
      "Step 400 Loss 0.000264279\n",
      "Step 500 Loss 0.000264936\n",
      "Step 600 Loss 0.000213346\n",
      "New data, epoch 38\n",
      "Step 0 Loss 0.190839\n",
      "Step 100 Loss 0.000227145\n",
      "Step 200 Loss 0.00027\n",
      "Step 300 Loss 0.000228743\n",
      "Step 400 Loss 0.000225077\n",
      "Step 500 Loss 0.000250926\n",
      "Step 600 Loss 0.00018956\n",
      "New data, epoch 39\n",
      "Step 0 Loss 0.162745\n",
      "Step 100 Loss 0.000236962\n",
      "Step 200 Loss 0.000214883\n",
      "Step 300 Loss 0.00022034\n",
      "Step 400 Loss 0.000253124\n",
      "Step 500 Loss 0.000217926\n",
      "Step 600 Loss 0.000215916\n",
      "New data, epoch 40\n",
      "Step 0 Loss 0.219463\n",
      "Step 100 Loss 0.000237586\n",
      "Step 200 Loss 0.000214765\n",
      "Step 300 Loss 0.000248529\n",
      "Step 400 Loss 0.000172834\n",
      "Step 500 Loss 0.000239518\n",
      "Step 600 Loss 0.000200986\n",
      "New data, epoch 41\n",
      "Step 0 Loss 0.127412\n",
      "Step 100 Loss 0.000204532\n",
      "Step 200 Loss 0.000199483\n",
      "Step 300 Loss 0.000202161\n",
      "Step 400 Loss 0.000210901\n",
      "Step 500 Loss 0.000237906\n",
      "Step 600 Loss 0.000140622\n",
      "New data, epoch 42\n",
      "Step 0 Loss 0.167994\n",
      "Step 100 Loss 0.000200645\n",
      "Step 200 Loss 0.000201055\n",
      "Step 300 Loss 0.000195618\n",
      "Step 400 Loss 0.000188093\n",
      "Step 500 Loss 0.000196108\n",
      "Step 600 Loss 0.000244696\n",
      "New data, epoch 43\n",
      "Step 0 Loss 0.159876\n",
      "Step 100 Loss 0.000285394\n",
      "Step 200 Loss 0.000206572\n",
      "Step 300 Loss 0.00015205\n",
      "Step 400 Loss 0.00017591\n",
      "Step 500 Loss 0.000192876\n",
      "Step 600 Loss 0.000151649\n",
      "New data, epoch 44\n",
      "Step 0 Loss 0.17737\n",
      "Step 100 Loss 0.000238883\n",
      "Step 200 Loss 0.00017226\n",
      "Step 300 Loss 0.000211052\n",
      "Step 400 Loss 0.000182167\n",
      "Step 500 Loss 0.000188412\n",
      "Step 600 Loss 0.000185416\n",
      "New data, epoch 45\n",
      "Step 0 Loss 0.19753\n",
      "Step 100 Loss 0.000173123\n",
      "Step 200 Loss 0.000257583\n",
      "Step 300 Loss 0.00019551\n",
      "Step 400 Loss 0.000212091\n",
      "Step 500 Loss 0.000180805\n",
      "Step 600 Loss 0.00017588\n",
      "New data, epoch 46\n",
      "Step 0 Loss 0.174276\n",
      "Step 100 Loss 0.000285191\n",
      "Step 200 Loss 0.000190539\n",
      "Step 300 Loss 0.000243759\n",
      "Step 400 Loss 0.00020653\n",
      "Step 500 Loss 0.000213986\n",
      "Step 600 Loss 0.000175815\n",
      "New data, epoch 47\n",
      "Step 0 Loss 0.184296\n",
      "Step 100 Loss 0.000198817\n",
      "Step 200 Loss 0.000201379\n",
      "Step 300 Loss 0.000191261\n",
      "Step 400 Loss 0.000182452\n",
      "Step 500 Loss 0.00018473\n",
      "Step 600 Loss 0.000130279\n",
      "New data, epoch 48\n",
      "Step 0 Loss 0.210347\n",
      "Step 100 Loss 0.000201564\n",
      "Step 200 Loss 0.000164965\n",
      "Step 300 Loss 0.000195505\n",
      "Step 400 Loss 0.000189332\n",
      "Step 500 Loss 0.000131003\n",
      "Step 600 Loss 0.000170751\n",
      "New data, epoch 49\n",
      "Step 0 Loss 0.196282\n",
      "Step 100 Loss 0.000240172\n",
      "Step 200 Loss 0.000206145\n",
      "Step 300 Loss 0.000211495\n",
      "Step 400 Loss 0.000154401\n",
      "Step 500 Loss 0.000201552\n",
      "Step 600 Loss 0.000180433\n",
      "New data, epoch 50\n",
      "Step 0 Loss 0.228619\n",
      "Step 100 Loss 0.000268722\n",
      "Step 200 Loss 0.000246124\n",
      "Step 300 Loss 0.000158642\n",
      "Step 400 Loss 0.000178555\n",
      "Step 500 Loss 0.000203771\n",
      "Step 600 Loss 0.00019456\n",
      "New data, epoch 51\n",
      "Step 0 Loss 0.14963\n",
      "Step 100 Loss 0.000180102\n",
      "Step 200 Loss 0.000235842\n",
      "Step 300 Loss 0.000217898\n",
      "Step 400 Loss 0.000156231\n",
      "Step 500 Loss 0.000156956\n",
      "Step 600 Loss 0.000172713\n",
      "New data, epoch 52\n",
      "Step 0 Loss 0.172635\n",
      "Step 100 Loss 0.000209436\n",
      "Step 200 Loss 0.00018983\n",
      "Step 300 Loss 0.000200671\n",
      "Step 400 Loss 0.000131005\n",
      "Step 500 Loss 0.000128688\n",
      "Step 600 Loss 0.000154431\n",
      "New data, epoch 53\n",
      "Step 0 Loss 0.132134\n",
      "Step 100 Loss 0.000179495\n",
      "Step 200 Loss 0.000169312\n",
      "Step 300 Loss 0.000179069\n",
      "Step 400 Loss 0.000171531\n",
      "Step 500 Loss 0.000136864\n",
      "Step 600 Loss 0.000181345\n",
      "New data, epoch 54\n",
      "Step 0 Loss 0.159003\n",
      "Step 100 Loss 0.000196033\n",
      "Step 200 Loss 0.000192333\n",
      "Step 300 Loss 0.000202306\n",
      "Step 400 Loss 0.000144372\n",
      "Step 500 Loss 0.000152517\n",
      "Step 600 Loss 0.000124349\n",
      "New data, epoch 55\n",
      "Step 0 Loss 0.188637\n",
      "Step 100 Loss 0.000141597\n",
      "Step 200 Loss 0.000153265\n",
      "Step 300 Loss 0.000164003\n",
      "Step 400 Loss 0.000179819\n",
      "Step 500 Loss 0.00019848\n",
      "Step 600 Loss 0.000164208\n",
      "New data, epoch 56\n",
      "Step 0 Loss 0.149189\n",
      "Step 100 Loss 0.000153322\n",
      "Step 200 Loss 0.000148821\n",
      "Step 300 Loss 0.000153103\n",
      "Step 400 Loss 0.000160871\n",
      "Step 500 Loss 0.000116693\n",
      "Step 600 Loss 0.000149263\n",
      "New data, epoch 57\n",
      "Step 0 Loss 0.127048\n",
      "Step 100 Loss 0.000183785\n",
      "Step 200 Loss 0.000144586\n",
      "Step 300 Loss 0.00013756\n",
      "Step 400 Loss 0.000147137\n",
      "Step 500 Loss 0.000172131\n",
      "Step 600 Loss 0.000142669\n",
      "New data, epoch 58\n",
      "Step 0 Loss 0.181986\n",
      "Step 100 Loss 0.000177459\n",
      "Step 200 Loss 0.000155882\n",
      "Step 300 Loss 0.000188789\n",
      "Step 400 Loss 0.000206219\n",
      "Step 500 Loss 0.000159961\n",
      "Step 600 Loss 0.000218639\n",
      "New data, epoch 59\n",
      "Step 0 Loss 0.122434\n",
      "Step 100 Loss 0.000113026\n",
      "Step 200 Loss 0.000155276\n",
      "Step 300 Loss 0.000154029\n",
      "Step 400 Loss 0.00016545\n",
      "Step 500 Loss 0.00011556\n",
      "Step 600 Loss 0.000154588\n",
      "New data, epoch 60\n",
      "Step 0 Loss 0.178634\n",
      "Step 100 Loss 0.000149432\n",
      "Step 200 Loss 0.000155397\n",
      "Step 300 Loss 0.000163406\n",
      "Step 400 Loss 0.000117825\n",
      "Step 500 Loss 0.000137934\n",
      "Step 600 Loss 0.000122336\n",
      "New data, epoch 61\n",
      "Step 0 Loss 0.150758\n",
      "Step 100 Loss 0.000140593\n",
      "Step 200 Loss 0.000124874\n",
      "Step 300 Loss 0.000145329\n",
      "Step 400 Loss 0.000144622\n",
      "Step 500 Loss 0.000117607\n",
      "Step 600 Loss 0.000110347\n",
      "New data, epoch 62\n",
      "Step 0 Loss 0.161787\n",
      "Step 100 Loss 0.00018799\n",
      "Step 200 Loss 0.000130569\n",
      "Step 300 Loss 0.000157398\n",
      "Step 400 Loss 0.00013647\n",
      "Step 500 Loss 0.000140717\n",
      "Step 600 Loss 0.000138478\n",
      "New data, epoch 63\n",
      "Step 0 Loss 0.149966\n",
      "Step 100 Loss 0.000152935\n",
      "Step 200 Loss 0.000163233\n",
      "Step 300 Loss 0.00012695\n",
      "Step 400 Loss 9.85474e-05\n",
      "Step 500 Loss 0.000128516\n",
      "Step 600 Loss 0.000119934\n",
      "New data, epoch 64\n",
      "Step 0 Loss 0.180284\n",
      "Step 100 Loss 0.000129294\n",
      "Step 200 Loss 0.000146678\n",
      "Step 300 Loss 0.000150943\n",
      "Step 400 Loss 0.000118255\n",
      "Step 500 Loss 0.000138066\n",
      "Step 600 Loss 0.000118654\n",
      "New data, epoch 65\n",
      "Step 0 Loss 0.231843\n",
      "Step 100 Loss 0.000130214\n",
      "Step 200 Loss 0.000231384\n",
      "Step 300 Loss 0.00016114\n",
      "Step 400 Loss 0.000181326\n",
      "Step 500 Loss 0.000129726\n",
      "Step 600 Loss 0.000142492\n",
      "New data, epoch 66\n",
      "Step 0 Loss 0.168442\n",
      "Step 100 Loss 0.000143522\n",
      "Step 200 Loss 0.000156598\n",
      "Step 300 Loss 0.000126894\n",
      "Step 400 Loss 0.000112106\n",
      "Step 500 Loss 0.000116255\n",
      "Step 600 Loss 0.000138112\n",
      "New data, epoch 67\n",
      "Step 0 Loss 0.15785\n",
      "Step 100 Loss 0.00014925\n",
      "Step 200 Loss 0.000157738\n",
      "Step 300 Loss 0.000133337\n",
      "Step 400 Loss 0.000144553\n",
      "Step 500 Loss 0.000143403\n",
      "Step 600 Loss 0.000107096\n",
      "New data, epoch 68\n",
      "Step 0 Loss 0.189404\n",
      "Step 100 Loss 0.000116577\n",
      "Step 200 Loss 0.000106417\n",
      "Step 300 Loss 0.000132589\n",
      "Step 400 Loss 0.000109751\n",
      "Step 500 Loss 0.000122878\n",
      "Step 600 Loss 0.000136032\n",
      "New data, epoch 69\n",
      "Step 0 Loss 0.140834\n",
      "Step 100 Loss 0.000135441\n",
      "Step 200 Loss 9.87897e-05\n",
      "Step 300 Loss 0.000147182\n",
      "Step 400 Loss 9.9326e-05\n",
      "Step 500 Loss 0.000118618\n",
      "Step 600 Loss 0.0001257\n",
      "New data, epoch 70\n",
      "Step 0 Loss 0.158275\n",
      "Step 100 Loss 0.000173328\n",
      "Step 200 Loss 0.000151243\n",
      "Step 300 Loss 0.000125927\n",
      "Step 400 Loss 0.000174699\n",
      "Step 500 Loss 0.000121569\n",
      "Step 600 Loss 0.000118557\n",
      "New data, epoch 71\n",
      "Step 0 Loss 0.119186\n",
      "Step 100 Loss 0.000149704\n",
      "Step 200 Loss 0.000108359\n",
      "Step 300 Loss 0.000136649\n",
      "Step 400 Loss 0.000134147\n",
      "Step 500 Loss 0.000108647\n",
      "Step 600 Loss 0.00010849\n",
      "New data, epoch 72\n",
      "Step 0 Loss 0.269486\n",
      "Step 100 Loss 0.000159754\n",
      "Step 200 Loss 0.000132678\n",
      "Step 300 Loss 0.000154789\n",
      "Step 400 Loss 0.000119989\n",
      "Step 500 Loss 9.34325e-05\n",
      "Step 600 Loss 9.6296e-05\n",
      "New data, epoch 73\n",
      "Step 0 Loss 0.25793\n",
      "Step 100 Loss 0.000116107\n",
      "Step 200 Loss 0.000147983\n",
      "Step 300 Loss 8.7196e-05\n",
      "Step 400 Loss 0.00010298\n",
      "Step 500 Loss 9.03017e-05\n",
      "Step 600 Loss 0.000113033\n",
      "New data, epoch 74\n",
      "Step 0 Loss 0.143487\n",
      "Step 100 Loss 0.000128485\n",
      "Step 200 Loss 0.000118026\n",
      "Step 300 Loss 0.00011931\n",
      "Step 400 Loss 0.000103621\n",
      "Step 500 Loss 0.000125077\n",
      "Step 600 Loss 0.000138667\n",
      "New data, epoch 75\n",
      "Step 0 Loss 0.175427\n",
      "Step 100 Loss 0.000148807\n",
      "Step 200 Loss 0.000120829\n",
      "Step 300 Loss 0.000102345\n",
      "Step 400 Loss 0.000119257\n",
      "Step 500 Loss 8.11033e-05\n",
      "Step 600 Loss 8.38594e-05\n",
      "New data, epoch 76\n",
      "Step 0 Loss 0.126991\n",
      "Step 100 Loss 0.000123499\n",
      "Step 200 Loss 0.00010843\n",
      "Step 300 Loss 0.000114298\n",
      "Step 400 Loss 9.76166e-05\n",
      "Step 500 Loss 9.34822e-05\n",
      "Step 600 Loss 8.15232e-05\n",
      "New data, epoch 77\n",
      "Step 0 Loss 0.155394\n",
      "Step 100 Loss 0.000120971\n",
      "Step 200 Loss 0.000179536\n",
      "Step 300 Loss 0.000133997\n",
      "Step 400 Loss 0.000120666\n",
      "Step 500 Loss 0.000110343\n",
      "Step 600 Loss 9.38772e-05\n",
      "New data, epoch 78\n",
      "Step 0 Loss 0.124357\n",
      "Step 100 Loss 0.000119205\n",
      "Step 200 Loss 0.000116847\n",
      "Step 300 Loss 0.000103765\n",
      "Step 400 Loss 7.6772e-05\n",
      "Step 500 Loss 8.52508e-05\n",
      "Step 600 Loss 9.85486e-05\n",
      "New data, epoch 79\n",
      "Step 0 Loss 0.21635\n",
      "Step 100 Loss 9.66091e-05\n",
      "Step 200 Loss 9.14185e-05\n",
      "Step 300 Loss 9.8535e-05\n",
      "Step 400 Loss 9.42068e-05\n",
      "Step 500 Loss 9.59454e-05\n",
      "Step 600 Loss 9.43358e-05\n",
      "New data, epoch 80\n",
      "Step 0 Loss 0.110861\n",
      "Step 100 Loss 0.001139\n",
      "Step 200 Loss 0.000114769\n",
      "Step 300 Loss 9.07525e-05\n",
      "Step 400 Loss 0.000109059\n",
      "Step 500 Loss 0.000184295\n",
      "Step 600 Loss 0.000110916\n",
      "New data, epoch 81\n",
      "Step 0 Loss 0.195396\n",
      "Step 100 Loss 0.000141462\n",
      "Step 200 Loss 9.6287e-05\n",
      "Step 300 Loss 0.000116343\n",
      "Step 400 Loss 0.000119215\n",
      "Step 500 Loss 0.000125359\n",
      "Step 600 Loss 9.288e-05\n",
      "New data, epoch 82\n",
      "Step 0 Loss 0.266211\n",
      "Step 100 Loss 0.000125605\n",
      "Step 200 Loss 0.000127092\n",
      "Step 300 Loss 0.000129926\n",
      "Step 400 Loss 0.000139182\n",
      "Step 500 Loss 0.000136259\n",
      "Step 600 Loss 0.000101678\n",
      "New data, epoch 83\n",
      "Step 0 Loss 0.188766\n",
      "Step 100 Loss 0.000125593\n",
      "Step 200 Loss 0.000113971\n",
      "Step 300 Loss 0.00010287\n",
      "Step 400 Loss 0.000110556\n",
      "Step 500 Loss 0.000110868\n",
      "Step 600 Loss 0.000117755\n",
      "New data, epoch 84\n",
      "Step 0 Loss 0.180517\n",
      "Step 100 Loss 0.00010471\n",
      "Step 200 Loss 0.000101235\n",
      "Step 300 Loss 0.000125211\n",
      "Step 400 Loss 0.00010769\n",
      "Step 500 Loss 8.85508e-05\n",
      "Step 600 Loss 0.0001055\n",
      "New data, epoch 85\n",
      "Step 0 Loss 0.13207\n",
      "Step 100 Loss 0.000113905\n",
      "Step 200 Loss 0.000103006\n",
      "Step 300 Loss 9.88432e-05\n",
      "Step 400 Loss 0.000108992\n",
      "Step 500 Loss 9.54048e-05\n",
      "Step 600 Loss 9.86408e-05\n",
      "New data, epoch 86\n",
      "Step 0 Loss 0.205882\n",
      "Step 100 Loss 9.65914e-05\n",
      "Step 200 Loss 8.68105e-05\n",
      "Step 300 Loss 0.000116568\n",
      "Step 400 Loss 8.29527e-05\n",
      "Step 500 Loss 0.000101992\n",
      "Step 600 Loss 7.9389e-05\n",
      "New data, epoch 87\n",
      "Step 0 Loss 0.110001\n",
      "Step 100 Loss 0.000102173\n",
      "Step 200 Loss 0.000126897\n",
      "Step 300 Loss 0.000108851\n",
      "Step 400 Loss 0.000108262\n",
      "Step 500 Loss 9.42489e-05\n",
      "Step 600 Loss 0.000103327\n",
      "New data, epoch 88\n",
      "Step 0 Loss 0.183177\n",
      "Step 100 Loss 9.75259e-05\n",
      "Step 200 Loss 0.000126844\n",
      "Step 300 Loss 0.00011073\n",
      "Step 400 Loss 0.000100708\n",
      "Step 500 Loss 9.12656e-05\n",
      "Step 600 Loss 8.37712e-05\n",
      "New data, epoch 89\n",
      "Step 0 Loss 0.172099\n",
      "Step 100 Loss 0.000111976\n",
      "Step 200 Loss 9.72036e-05\n",
      "Step 300 Loss 0.000131089\n",
      "Step 400 Loss 8.33585e-05\n",
      "Step 500 Loss 8.52351e-05\n",
      "Step 600 Loss 9.30931e-05\n",
      "New data, epoch 90\n",
      "Step 0 Loss 0.150391\n",
      "Step 100 Loss 9.75223e-05\n",
      "Step 200 Loss 8.38833e-05\n",
      "Step 300 Loss 0.000133066\n",
      "Step 400 Loss 0.000104657\n",
      "Step 500 Loss 0.000109182\n",
      "Step 600 Loss 9.7396e-05\n",
      "New data, epoch 91\n",
      "Step 0 Loss 0.163679\n",
      "Step 100 Loss 8.11845e-05\n",
      "Step 200 Loss 8.39309e-05\n",
      "Step 300 Loss 7.40931e-05\n",
      "Step 400 Loss 0.00010009\n",
      "Step 500 Loss 0.000116537\n",
      "Step 600 Loss 8.15182e-05\n",
      "New data, epoch 92\n",
      "Step 0 Loss 0.179159\n",
      "Step 100 Loss 8.39275e-05\n",
      "Step 200 Loss 8.18386e-05\n",
      "Step 300 Loss 7.15869e-05\n",
      "Step 400 Loss 9.96264e-05\n",
      "Step 500 Loss 0.00010327\n",
      "Step 600 Loss 9.46638e-05\n",
      "New data, epoch 93\n",
      "Step 0 Loss 0.0944319\n",
      "Step 100 Loss 8.24312e-05\n",
      "Step 200 Loss 8.31792e-05\n",
      "Step 300 Loss 9.89825e-05\n",
      "Step 400 Loss 8.89326e-05\n",
      "Step 500 Loss 7.74087e-05\n",
      "Step 600 Loss 8.70598e-05\n",
      "New data, epoch 94\n",
      "Step 0 Loss 0.226451\n",
      "Step 100 Loss 0.000118353\n",
      "Step 200 Loss 9.1317e-05\n",
      "Step 300 Loss 9.64464e-05\n",
      "Step 400 Loss 9.72197e-05\n",
      "Step 500 Loss 8.5712e-05\n",
      "Step 600 Loss 9.74823e-05\n",
      "New data, epoch 95\n",
      "Step 0 Loss 0.127095\n",
      "Step 100 Loss 8.16875e-05\n",
      "Step 200 Loss 7.18506e-05\n",
      "Step 300 Loss 7.31744e-05\n",
      "Step 400 Loss 7.64347e-05\n",
      "Step 500 Loss 6.96062e-05\n",
      "Step 600 Loss 8.17776e-05\n",
      "New data, epoch 96\n",
      "Step 0 Loss 0.141212\n",
      "Step 100 Loss 8.12983e-05\n",
      "Step 200 Loss 9.44562e-05\n",
      "Step 300 Loss 9.24755e-05\n",
      "Step 400 Loss 9.44094e-05\n",
      "Step 500 Loss 0.000110459\n",
      "Step 600 Loss 7.74018e-05\n",
      "New data, epoch 97\n",
      "Step 0 Loss 0.118805\n",
      "Step 100 Loss 8.30116e-05\n",
      "Step 200 Loss 7.75989e-05\n",
      "Step 300 Loss 7.37424e-05\n",
      "Step 400 Loss 9.17454e-05\n",
      "Step 500 Loss 8.94631e-05\n",
      "Step 600 Loss 9.46946e-05\n",
      "New data, epoch 98\n",
      "Step 0 Loss 0.181217\n",
      "Step 100 Loss 0.000106138\n",
      "Step 200 Loss 8.74844e-05\n",
      "Step 300 Loss 8.83886e-05\n",
      "Step 400 Loss 7.67723e-05\n",
      "Step 500 Loss 7.66125e-05\n",
      "Step 600 Loss 0.000108695\n",
      "New data, epoch 99\n",
      "Step 0 Loss 0.103032\n",
      "Step 100 Loss 9.37677e-05\n",
      "Step 200 Loss 8.72444e-05\n",
      "Step 300 Loss 0.000101494\n",
      "Step 400 Loss 9.61034e-05\n",
      "Step 500 Loss 8.35761e-05\n",
      "Step 600 Loss 6.35535e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09db7e1fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 3 Training the network\n",
    "with tf.Session() as sess:\n",
    "    #we stupidly have to do this everytime, it should just know\n",
    "    #that we initialized these vars. v2 guys, v2..\n",
    "    sess.run(tf.global_variables_initializer)\n",
    "    #interactive mode\n",
    "    plt.ion()\n",
    "    #initialize the figure\n",
    "    plt.figure()\n",
    "    #show the graph\n",
    "    plt.show()\n",
    "    #to show the loss decrease\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        #generate data at eveery epoch, batches run in epochs\n",
    "        x,y = generateData()\n",
    "        #initialize an empty hidden state\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "        #each batch\n",
    "        for batch_idx in range(num_batches):\n",
    "            #starting and ending point per batch\n",
    "            #since weights reoccuer at every layer through time\n",
    "            #These layers will not be unrolled to the beginning of time, \n",
    "            #that would be too computationally expensive, and are therefore truncated \n",
    "            #at a limited number of time-steps\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            #run the computation graph, give it the values\n",
    "            #we calculated earlier\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that we are moving truncated_backprop_length steps forward on each iteration (line 15–19), but it is possible have different strides. This subject is further elaborated in this article. The downside with doing this is that truncated_backprop_length need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data. Otherwise there might a lot of “misses”, as you can see on the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*uKuUKp_m55zAPCzaIemucA.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*uKuUKp_m55zAPCzaIemucA.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Time series of squares, the elevated black square symbolizes an echo-output, which is activated three steps from the echo input (black square). The sliding batch window is also striding three steps at each run, which in our sample case means that no batch will encapsulate the dependency, so it can not train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The network will be able to exactly learn the echo behavior so there is no need for testing data.\n",
    "The program will update the plot as training progresses, Blue bars denote a training input signal (binary one), red bars show echos in the training output and green bars are the echos the net is generating. The different bar plots show different sample series in the current batch. Fully trained at 100 epochs look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ytquMdmGMJo0-3kxMCi1Gg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*ytquMdmGMJo0-3kxMCi1Gg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
